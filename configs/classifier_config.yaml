model:
  base_model: microsoft/deberta-v3-large
  num_classes: 4
  hidden_dropout_prob: 0.1
  attention_probs_dropout_prob: 0.1
  target_parameters: 304_000_000

training:
  learning_rate: 1.0e-5
  lr_schedule: linear
  warmup_ratio: 0.06
  batchwksize: 16
  gradient_accumulation_steps: 2
  effective_batch_size: 32
  num_epochs: 5
  optimizer: AdamW
  weight_decay: 0.01
  gradient_clip_norm: 1.0

data:
  train_size: 14_650
  val_split: 0.1
  max_length: 512

loss:
  function: crossentropy
  label_smoothing: 0.1

evaluation:
  metrics: [macro_f1, precision, recall, accuracy]
  target_macro_f1: 0.89
  per_class_targets:
    L1_f1: 0.91
    L2_f1: 0.88
    L3_f1: 0.87
    L4_f1: 0.86

cascade_routing:
  confidence_threshold: 0.7
  fallback_to_full_verification: true

checkpoint:
  save_best_only: true
  monitor: val_macro_f1
  mode: max

logging:
  log_every_n_steps: 100
  use_wandb: false
  project_name: lcr-classifier
