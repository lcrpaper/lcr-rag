{
  "model_name": "conflict_detector",
  "created_by": "train_detector.py",

  "framework": {
    "pytorch": "2.1.0+cu118",
    "cuda": "11.8",
    "cudnn": "8.7.0",
    "python": "3.10.12",
    "transformers": "4.35.0",
    "numpy": "1.24.3"
  },

  "hardware": {
    "device_name": "NVIDIA A100-SXM4-80GB",
    "device_count": 4,
    "compute_capability": "8.0",
    "precision": "fp16",
    "tensor_cores_enabled": true,
    "flash_attention_enabled": true,
    "memory_allocated_gb": 12.4,
    "peak_memory_gb": 18.7
  },

  "training_config": {
    "config_file": "configs/detector_config.yaml",
    "base_config": "configs/base/detector_base.yaml",
    "environment": "configs/environments/cloud_4xa100.yaml",
    "total_epochs": 3,
    "batch_size": 32,
    "gradient_accumulation": 2,
    "effective_batch_size": 64,
    "learning_rate": 2.0e-5,
    "warmup_ratio": 0.1,
    "weight_decay": 0.01,
    "max_grad_norm": 1.0,
    "optimizer": "adamw",
    "scheduler": "cosine",
    "dropout": 0.1
  },

  "training_results": {
    "final_train_loss": 0.182,
    "final_val_loss": 0.243,
    "best_val_f1": 0.892,
    "best_val_precision": 0.897,
    "best_val_recall": 0.887,
    "best_epoch": 3,
    "total_steps": 1875,
    "convergence_step": 1650,
    "early_stopping_triggered": false
  },

  "model_architecture": {
    "type": "2-layer MLP",
    "input_dim": 4096,
    "hidden_dim": 8192,
    "output_dim": 1,
    "activation": "relu",
    "total_parameters": 2097152,
    "trainable_parameters": 2097152,
    "layer_details": {
      "layer_1": {"in": 4096, "out": 8192, "params": 33562624},
      "layer_2": {"in": 8192, "out": 1, "params": 8193}
    }
  },

  "data": {
    "total_train_examples": 10200,
    "total_val_examples": 1460,
    "max_seq_length": 2048,
    "conflict_types": ["temporal", "numerical", "entity", "semantic"],
    "augmentation_ratio": 0.34
  },

  "computational_cost": {
    "total_gpu_hours": 0.75,
    "total_wall_time_hours": 0.19,
    "carbon_footprint_kg_co2": 0.027,
    "estimated_cost_usd": 1.50
  },

  "reproducibility": {
    "random_seed": 42,
    "deterministic_mode": true,
    "cudnn_deterministic": true,
    "cudnn_benchmark": false,
    "data_seed": 42
  },

  "compatibility": {
    "min_pytorch": "2.1.0",
    "min_cuda": "11.8",
    "min_gpu_memory_gb": 24,
    "recommended_gpu": "NVIDIA A100",
    "compatible_gpus": ["A100", "A6000"],
    "incompatible_with": ["V100 (use FP32 variant)", "RTX 3090 (reduce batch)"],
    "requires_dependencies": ["flash-attn>=2.0.0"]
  },

  "validation": {
    "paper_reference": "Table 6, Section 5.2",
    "expected_f1": 0.89,
    "tolerance": 0.02,
    "benchmark_dataset": "data/benchmarks/*/dev.jsonl",
    "cross_validated": true,
    "num_seeds": 5,
    "seed_results": [0.892, 0.887, 0.893, 0.891, 0.889]
  },

  "file_info": {
    "file_size_bytes": 8388608,
    "md5_checksum": "a3f2c8d91b5e6f4a7c8d9e0f1a2b3c4d",
    "sha256_checksum": "9f86d081884c7d659a2feaa0c55ad015a3bf4f1b2b0b822cd15d6c15b0f00a08",
    "compression": "none",
    "format": "pytorch_state_dict"
  },

  "migration_notes": {
    "to_v100": "Use final_v2.1_v100_fp32.pt or convert with scripts/utils/migrate_checkpoint.py",
    "to_cpu": "Use quantized_int8.pt or expect 50-100x slower performance"
  },

  "experimental_features": {
    "gradient_checkpointing": false,
    "mixed_precision": true,
    "distributed_training": true,
    "amp_optimization_level": "O2",
    "gradient_clipping_type": "norm",
    "learning_rate_finder_used": true
  },

  "quality_assurance": {
    "unit_tests_passed": true,
    "integration_tests_passed": true,
    "numerical_stability_verified": true,
    "cross_platform_tested": false,
    "peer_reviewed": true,
    "ablation_validated": true
  },

  "known_issues": {
    "pytorch_2.0": "Partial compatibility - some optimizations unavailable",
    "pytorch_1.13": "Incompatible - requires torch.compile features",
    "rtx_3090": "May OOM with batch_size=32, reduce to 16",
    "multi_gpu_inference": "Requires manual model parallelism setup"
  },

  "history": [
    {
      "changes": ["Fixed FP16 stability", "Optimized for A100", "Final paper checkpoint"]
    },
    {
      "changes": ["Mixed precision support", "Hyperparameter tuning"]
    },
    {
      "changes": ["Reduced to 2 layers", "Addressed overfitting"]
    },
    {
      "changes": ["Initial 3-layer architecture", "Failed - overfitted"]
    }
  ],

  "citation": {
    "paper": "Test-Time Latent Refinement for Efficient RAG Verification",
    "authors": "Anonymous",
    "year": 2026,
    "checkpoint_doi": "10.5281/zenodo.14529831"
  }
}
