_schema_version: "1.0"
_config_type: "environment"
_extends: null

hardware:
  name: "cloud_4xa100"
  description: "4x NVIDIA A100-SXM4-80GB on cloud instance"

  device: "cuda"
  device_count: 4

  gpu:
    name: "NVIDIA A100-SXM4-80GB"
    memory_gb: 80
    compute_capability: "8.0"
    tensor_cores: true
    nvlink: true
    nvlink_version: 3

  cpu:
    cores: 48
    threads: 96

  memory:
    system_ram_gb: 512
    swap_gb: 0

  interconnect:
    type: "NVLink"
    bandwidth_gbps: 600

distributed:
  enabled: true
  backend: "nccl"
  world_size: 4

  strategy:
    name: "ddp"
    find_unused_parameters: false
    gradient_as_bucket_view: true
    static_graph: true

  fsdp:
    enabled: false
    sharding_strategy: "FULL_SHARD"
    cpu_offload: false

optimizations:
  memory:
    gradient_checkpointing: false
    activation_checkpointing: false

  precision:
    training: "float16"
    inference: "float16"
    amp_enabled: true
    amp_opt_level: "O2"

  batch_overrides:
    detector:
      train_batch_size: 32
      gradient_accumulation_steps: 2
    refinement:
      train_batch_size: 16
      gradient_accumulation_steps: 4
    classifier:
      train_batch_size: 16
      gradient_accumulation_steps: 2

  torch_compile:
    enabled: true
    mode: "reduce-overhead"
    fullgraph: false

  flash_attention:
    enabled: true
    version: "2.0"

  cuda:
    matmul_precision: "high"
    allow_tf32: true
    cudnn_benchmark: false
    cudnn_deterministic: true

data:
  num_workers: 8
  pin_memory: true
  prefetch_factor: 4
  persistent_workers: true
  drop_last: true

cloud:
  provider: "gcp"
  instance_type: "a2-ultragpu-4g"
  region: "us-central1-a"
  spot_instance: true
  preemption_handler: true
